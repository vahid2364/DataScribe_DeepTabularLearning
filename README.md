# DataScribe Deep Tabular Learning

This repository provides the code, data, and documentation for the paper:

**[Decoding Non-Linearity and Complexity: Deep Tabular Learning Approaches for Materials Science](https://arxiv.org/abs/2411.18717)**  
*Vahid Attari, Raymundo Arroyave*  
Department of Materials Science & Engineering, Texas A&M University.

<div align="center">
   <img src="https://media.licdn.com/dms/image/v2/C4E12AQE98dfpdYhxZA/article-cover_image-shrink_600_2000/article-cover_image-shrink_600_2000/0/1610116669577?e=2147483647&v=beta&t=A7bKteJy6T0CUcImJUute3Bio5J_olkFKVXwgy-TMP8" alt="Work in Progress" width="200">
</div>

---

## ğŸ“– Overview

This project addresses the challenges in tabular materials science data, including extreme skewness, multimodal distributions, and complex feature interactions. The repository implements several machine learning models, ranging from classical tree-based methods (e.g., XGBoost) to advanced deep learning architectures (e.g., encoder-decoder models, TabNet, and Variational Autoencoders).

Key features of the project:
- **Tabular data preprocessing** for skewness and outliers.
- **Benchmarking of deep learning models** (e.g., Fully Dense Networks, Disjunctive Normal Form Networks, TabNet).
- **Bayesian optimization** for hyperparameter tuning.
- Comparative analysis of predictive performance and computational efficiency.

---

## ğŸ“‚ Repository Structure

```
.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                       # Raw input data files
â”‚   â”œâ”€â”€ processed/                 # Preprocessed data files
â”‚   â””â”€â”€ README.md                  # Details about datasets
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ encoder_decoder.py         # Encoder-Decoder implementation
â”‚   â”‚          â”œâ”€â”€ Fully Dense architecture
â”‚   â”‚          â”œâ”€â”€ DNNF architecture
â”‚   â”‚          â””â”€â”€ 1D-CNN architecture
â”‚   â”‚          
â”‚   â”œâ”€â”€ tabnet.py                  # TabNet model
â”‚   â”œâ”€â”€ vae.py                     # Variational Autoencoder implementation
â”‚   â”œâ”€â”€ xgboost_baseline.py        # XGBoost baseline model
â”‚   â””â”€â”€ utils.py                   # Utility functions for training and evaluation
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ figures/                   # Figures and plots
â”‚   â”œâ”€â”€ tables/                    # Results tables (e.g., metrics, hyperparameters)
â”‚   â””â”€â”€ README.md                  # Description of results
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb  # Exploratory Data Analysis (EDA)
â”‚   â”œâ”€â”€ 02_model_training.ipynb    # Model training and evaluation
â”‚   â”œâ”€â”€ 03_hyperparameter_tuning.ipynb  # Hyperparameter optimization using TPE
â”‚   â””â”€â”€ 04_results_visualization.ipynb  # Visualization of model performance
â”œâ”€â”€ README.md                      # Project documentation
â””â”€â”€ requirements.txt               # Python dependencies
```

---

## ğŸ”§ Installation

### Prerequisites
- Python 3.8+
- A GPU is recommended for training deep learning models.

### Steps
1. Clone the repository:
   ```bash
   git clone https://github.com/vahid2364/DataScribe_DeepTabularLearning.git
   cd DataScribe_DeepTabularLearning
   ```

2. Create and activate a virtual environment:
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`
   ```

3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

---

## ğŸ“Š Datasets

### Description
The dataset includes tabular materials data with features such as:
- **Compositional information**: Nb, Cr, V, W, Zr.
- **Material properties**: Thermal conductivity, density, yield strength, creep rate.

### Preprocessing
1. Missing values are handled via interpolation.
2. Features are normalized using quantile transformation for skewed distributions.

For more details, see the [data README](data/README.md).

---

## ğŸš€ Models

The repository implements and benchmarks the following models:
- **XGBoost**: Baseline tree-based model.
- **Fully Dense Neural Network (FDN)**: Standard fully connected architecture.
- **Disjunctive Normal Form Network (DNF-Net)**: Captures logical relationships in tabular data.
- **TabNet**: Attention-based feature selection for interpretability.
- **Variational Autoencoder (VAE)**: Generative model for handling uncertainty.

### Key Metrics
- **Mean Squared Error (MSE)**
- **R-squared (RÂ²)**
- **Symmetric Mean Absolute Percentage Error (SMAPE)**

See [models README](models/README.md) for implementation details.

---

## ğŸ” Results

1. **Performance Comparison**:
   - XGBoost achieves the best computational efficiency and strong predictive performance.
   - DNF-Net outperforms on skewed features but has higher computational costs.

2. **Optimization Insights**:
   - Bayesian optimization using Tree-structured Parzen Estimator (TPE) fine-tunes hyperparameters.
   - Scaling and quantile transformations significantly improve model stability.

Key results are summarized in the [results directory](results/).

---

## ğŸ§ª Usage

### Training a Model
Run the model training script:
```bash
python models/encoder_decoder.py --data data/processed/dataset.csv --epochs 50
```

### Visualizing Results
Use the provided Jupyter notebooks:
```bash
jupyter notebook notebooks/01_data_exploration.ipynb
```

---

## ğŸ› ï¸ Contributing

Contributions are welcome! Please follow these steps:
1. Fork the repository.
2. Create a feature branch: `git checkout -b feature-name`.
3. Commit changes: `git commit -m "Description of changes"`.
4. Push to your branch: `git push origin feature-name`.
5. Create a pull request.

---

## ğŸ“œ License

This project is licensed under the [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/).

---

## ğŸ“ Contact

For questions or issues, contact the authors:
- Vahid Attari: attari.v@tamu.edu


